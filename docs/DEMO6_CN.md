# 看，这是我用2个小时时间 vibecoding 的 AI 开发基础框架 🚀

> **Demo6: 基于队列的企业级 AI 对话系统**  
> 一个尝试用 AI 协作开发（vibecoding）构建的 AI 开发工作流基础框架

<div align="center">

**作者：Charlie Cao | AI 开发者 & Vibecoding 实践者**

[![GitHub](https://img.shields.io/badge/GitHub-charlie--cao-181717?style=flat&logo=github)](https://github.com/charlie-cao)
[![Bun](https://img.shields.io/badge/Bun-1.3+-000000?style=flat&logo=bun)](https://bun.sh)
[![TypeScript](https://img.shields.io/badge/TypeScript-5.0+-3178C6?style=flat&logo=typescript)](https://www.typescriptlang.org)

</div>

## 前言

大家好！我是 **Charlie Cao**，一个热衷于探索 AI 与软件开发交汇点的开发者。

最近我在尝试一种新的开发方式——**vibecoding**（和 AI 一起协作编程）。花了大概 2 个小时，用 Cursor + AI 助手一起构建了这个 Demo6 项目。想和大家分享一下这个过程，也希望能得到大家的指教和建议！

这个项目本质上是一个**企业级 AI 对话系统的基础框架**，核心思路是：**如何让 AI 开发工作流更加高效、可控、可扩展**。

### 关于 Vibecoding

**Vibecoding** 是我给这种开发方式起的名字：**和 AI 一起"共振"编程**。不是简单的"AI 写代码我复制"，而是：

- 🤝 **协作而非替代**：AI 是伙伴，不是替代品
- 🚀 **加速而非偷懒**：快速原型，但保持代码质量
- 🎓 **学习而非依赖**：在协作中学习新技术
- 🔄 **迭代而非完美**：快速试错，持续改进

如果你也在尝试类似的开发方式，欢迎交流！

## 🎯 项目目标

在 AI 时代，我们需要的不仅仅是"能跑"的代码，更需要：
- **可扩展的架构**：能轻松接入不同的 AI 模型
- **可控的流程**：任务队列、优先级、重试机制
- **实时的反馈**：用户能实时看到处理进度
- **企业级特性**：性能监控、错误处理、并发控制

这就是 Demo6 想要解决的问题。

## 🛠️ 技术栈

### 核心框架
- **Bun.js** - 高性能 JavaScript 运行时（比 Node.js 快 4x！）
- **BullMQ** - 基于 Redis 的现代队列系统
- **Redis** - 内存数据库，作为队列后端
- **Ollama** - 本地大语言模型服务（qwen3:latest）

### 前端技术
- **React** - UI 框架
- **TypeScript** - 类型安全
- **Tailwind CSS** - 样式框架
- **Shadcn UI** - 组件库
- **Server-Sent Events (SSE)** - 实时状态推送

### 为什么选这些技术？

1. **Bun.js**：启动快、原生支持 TypeScript、内置 WebSocket/SSE，非常适合做 AI 服务的后端
2. **BullMQ**：比传统的 Bull 更现代，支持优先级、延迟、重试，完美契合 AI 任务队列的需求
3. **Ollama**：本地运行，数据安全，而且免费！
4. **SSE**：比 WebSocket 更简单，适合单向推送场景（任务状态更新）

## 🏗️ 架构设计

```
┌─────────────────┐
│   React 前端     │  ← 用户界面，实时显示队列状态
│   (Port 3000)   │
└────────┬────────┘
         │ HTTP + SSE
         ↓
┌─────────────────┐
│   队列服务       │  ← Bun.js 服务器，处理任务入队
│  (Port 3001)    │     状态查询、SSE 推送
└────────┬────────┘
         │
         ↓
┌─────────────────┐
│     Redis       │  ← 存储队列数据、任务状态
│  (Port 6379)    │     支持持久化
└────────┬────────┘
         │
         ↓
┌─────────────────┐
│   Worker 进程   │  ← 异步处理任务
└────────┬────────┘
         │
         ↓
┌─────────────────┐
│    Ollama       │  ← 本地 LLM 服务
│  (Port 11434)   │     qwen3:latest 模型
└─────────────────┘
```

### 核心流程

1. **用户发送消息** → 前端发送到队列服务
2. **任务入队** → BullMQ 将任务存入 Redis
3. **Worker 处理** → 异步调用 Ollama 生成响应
4. **实时推送** → 通过 SSE 推送状态更新到前端
5. **结果显示** → 前端实时显示进度和结果

## ✨ 核心功能

### 1. 任务队列管理
- **等待/处理/完成** 状态追踪
- 支持**优先级队列**（高/中/低）
- **自动重试机制**（失败自动重试 3 次）
- **速率限制**（防止 API 过载）

### 2. 实时进度追踪
- **0-100%** 进度显示
- **队列位置**显示（"前面还有 X 个任务"）
- **实时状态更新**（通过 SSE）

### 3. 并发处理
- 同时处理 **3 个任务**
- 智能调度，高优先级任务优先

### 4. 流式响应
- **SSE 实时推送**，无需 WebSocket
- 自动重连机制

### 5. 性能监控
- **平均响应时间**
- **总请求数**
- **成功率**
- **吞吐量**（每分钟请求数）

### 6. 任务历史
- 记录最近 **50 条**任务
- 状态追踪（完成/失败/处理中）

## 🚀 快速开始

### 前置要求
- Bun.js (v1.3.3+)
- Docker (用于 Redis)
- Ollama (本地 LLM 服务)

### 安装步骤

1. **克隆项目**
```bash
git clone <your-repo>
cd one_company
```

2. **安装依赖**
```bash
bun install
```

3. **启动 Redis**
```bash
docker-compose -f docker-compose.demo6.yml up -d
```

4. **启动 Ollama**（如果还没运行）
```bash
ollama serve
# 确保 qwen3:latest 模型已下载
ollama pull qwen3:latest
```

5. **启动队列服务器**
```bash
bun run src/server/demo6-server.ts
```

6. **启动前端**
```bash
bun dev
```

7. **访问应用**
打开浏览器访问 `http://localhost:3000/demo6`

## 💡 使用体验

### 基本使用
1. 在输入框输入你的问题
2. 点击"发送"或按 Enter
3. 实时查看队列状态和处理进度
4. 等待 AI 响应

### 高级功能
- **AI 生成问题**：点击"AI 生成问题"按钮，让 AI 帮你生成问题
- **优先级设置**：在侧边栏设置任务优先级（低/中/高）
- **查看历史**：在侧边栏查看任务历史记录
- **性能监控**：实时查看系统性能指标

### 中英文切换
右上角有语言切换按钮，支持中英文界面切换。

## 🤖 Vibecoding 开发体验

### 什么是 Vibecoding？

Vibecoding 是我给这种开发方式起的名字：**和 AI 一起"共振"编程**。不是简单的"AI 写代码我复制"，而是：

1. **我描述需求** → AI 理解并给出方案
2. **AI 生成代码** → 我审查和优化
3. **遇到问题** → 一起调试和解决
4. **迭代改进** → 持续优化

### 开发时间线

- **0-30 分钟**：架构设计 + 基础框架搭建
- **30-60 分钟**：队列系统集成 + Worker 实现
- **60-90 分钟**：前端 UI + SSE 实时推送
- **90-120 分钟**：功能完善 + 中英文切换 + 错误处理

### 遇到的挑战

1. **CORS 问题**：前后端分离，需要正确配置 CORS
2. **SSE 连接管理**：需要处理连接断开、重连
3. **错误处理**：AbortError、网络错误等需要优雅处理
4. **状态同步**：前端状态和队列状态需要实时同步

### AI 协作的优势

- **快速原型**：2 小时完成一个完整系统
- **代码质量**：AI 生成的代码结构清晰，类型安全
- **学习效率**：边做边学，理解新技术栈
- **迭代速度**：快速试错，快速改进

## 📊 性能表现

- **启动时间**：< 1 秒（Bun.js 优势）
- **响应时间**：取决于 Ollama 模型速度（本地 qwen3 约 2-5 秒）
- **并发能力**：同时处理 3 个任务
- **吞吐量**：约 10-20 请求/分钟（取决于模型速度）

## 🔮 未来计划

### 短期（1-2 周）
- [ ] 支持更多 AI 模型（OpenAI、Anthropic 等）
- [ ] 添加用户认证系统
- [ ] 实现对话历史持久化
- [ ] 添加更多监控指标

### 中期（1-2 月）
- [ ] 支持批量任务处理
- [ ] 实现任务调度（定时任务）
- [ ] 添加 Webhook 支持
- [ ] 性能优化（缓存、连接池等）

### 长期（3-6 月）
- [ ] 分布式部署支持
- [ ] 多租户架构
- [ ] 插件系统
- [ ] 可视化工作流编辑器

## 🤝 求指教！

这个项目还在不断完善中，希望大家能：

1. **提建议**：架构设计、代码优化、功能改进
2. **报 Bug**：如果发现任何问题，欢迎提 Issue
3. **分享经验**：如果你也在做类似的项目，欢迎交流
4. **Star 支持**：如果觉得有用，给个 Star 吧！

### 特别想了解的问题

- **架构设计**：这样的架构是否合理？有没有更好的方案？
- **性能优化**：如何进一步提升并发能力和响应速度？
- **错误处理**：企业级应用还需要考虑哪些错误场景？
- **扩展性**：如何更好地支持多模型、多租户？

## 📝 技术细节

### 队列配置
- **默认优先级**：5（中等）
- **最大重试次数**：3
- **并发数**：3
- **速率限制**：10 请求/分钟

### API 端点
- `POST /api/chat` - 提交聊天任务
- `GET /api/queue/stats` - 获取队列统计
- `GET /stream/status/:jobId` - SSE 状态流
- `GET /health` - 健康检查

### 数据流
```
用户输入 → 队列服务 → Redis → Worker → Ollama → 结果 → SSE → 前端
```

## 🎓 学习资源

如果你想深入了解这些技术：

- [Bun.js 官方文档](https://bun.sh/docs)
- [BullMQ 文档](https://docs.bullmq.io/)
- [Ollama 文档](https://ollama.ai/docs)
- [Server-Sent Events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events)

## 📄 许可证

MIT License

## 🙏 致谢

- **Cursor AI** - 强大的 AI 编程助手
- **Bun.js 团队** - 优秀的 JavaScript 运行时
- **BullMQ 团队** - 强大的队列系统
- **Ollama 团队** - 本地 LLM 解决方案

---

**最后，再次感谢大家的关注！如果这个项目对你有帮助，或者你想一起完善它，欢迎交流！** 🚀

*P.S. 如果你也在尝试 vibecoding，欢迎分享你的经验和心得！*

